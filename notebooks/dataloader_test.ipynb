{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paolo/git/spotify-playlist-generator/venv/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Any\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "import torchaudio.transforms as T\n",
    "from safetensors import safe_open\n",
    "from torch import nn\n",
    "from torch_audiomentations import (\n",
    "    Compose,\n",
    "    OneOf,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "import os\n",
    "from time import perf_counter\n",
    "\n",
    "SEED = 654\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "seed_everything(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T12:27:56.920865560Z",
     "start_time": "2023-12-31T12:27:52.958018808Z"
    }
   },
   "id": "15d413b70e9b2b49"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_splits(\n",
    "        data: pd.DataFrame | np.ndarray | list[...],\n",
    "        train_size: float,\n",
    "        valid_size: float,\n",
    "        test_size: float,\n",
    "        stratify_col: str | None = None,\n",
    ") -> tuple[Any, Any, Any]:\n",
    "    assert train_size + valid_size + test_size <= 1.0\n",
    "\n",
    "    if stratify_col:\n",
    "        train_split, valid_test = train_test_split(\n",
    "            data, train_size=train_size, stratify=data[stratify_col], random_state=SEED\n",
    "        )\n",
    "        valid_split, test_split = train_test_split(\n",
    "            valid_test, train_size=valid_size / (1 - train_size), stratify=valid_test[stratify_col], random_state=SEED\n",
    "        )\n",
    "    else:\n",
    "        train_split, valid_test = train_test_split(data, train_size=train_size, stratify=None, random_state=SEED)\n",
    "        valid_split, test_split = train_test_split(\n",
    "            valid_test, train_size=valid_size / (1 - train_size), stratify=None, random_state=SEED\n",
    "        )\n",
    "\n",
    "    return train_split, valid_split, test_split\n",
    "\n",
    "class AddGaussianNoise(torch.nn.Module):\n",
    "    def __init__(self, mean: float = 0.0, std: float = 1.0, p: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        assert 0 <= p <= 1\n",
    "        self.std: float = std\n",
    "        self.mean: float = mean\n",
    "        self.p: float = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + torch.randn(x.size()) * self.std + self.mean if random.random() < self.p else x\n",
    "\n",
    "\n",
    "class MinMaxNorm(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        new_min, new_max = 0.0, 1.0\n",
    "        return (x - x_min) / (x_max - x_min) * (new_max - new_min) + new_min\n",
    "\n",
    "\n",
    "songs_path: list[pathlib.Path] = list(pathlib.Path(os.getcwd()).parent.rglob(\"*.safetensors\"))\n",
    "train, valid, test = get_splits(songs_path, train_size=0.7, valid_size=0.2, test_size=0.1, stratify_col=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T12:27:57.797072117Z",
     "start_time": "2023-12-31T12:27:56.927057292Z"
    }
   },
   "id": "e7be577441cc3be0"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            data_path: np.ndarray | list[str],\n",
    "            image_size: int,\n",
    "            sample_rate: int = 44100,\n",
    "            crop_size: int = 30,\n",
    "            mode: str = \"train\",\n",
    "    ) -> None:\n",
    "        assert mode in {\"train\", \"valid\", \"test\"}\n",
    "        super().__init__()\n",
    "        self.data_path: np.ndarray | list[str] = data_path\n",
    "        self.image_size: int = image_size\n",
    "        self.sample_rate: int = sample_rate\n",
    "        self.crop_size: int = crop_size\n",
    "        self.mode: str = mode\n",
    "        # self._init_transforms()\n",
    "\n",
    "    def _get_transforms(self, sample_rate) -> tuple[Compose, Compose]:\n",
    "        transforms = [\n",
    "            T.MelSpectrogram(\n",
    "                sample_rate=sample_rate,\n",
    "                n_fft=512,\n",
    "                win_length=512,\n",
    "                hop_length=256,\n",
    "                n_mels=256,\n",
    "                normalized=True,\n",
    "            ),\n",
    "            v2.Resize(size=(self.image_size, self.image_size)),\n",
    "            MinMaxNorm(),\n",
    "            v2.ToDtype(torch.float16, scale=False),\n",
    "        ]\n",
    "\n",
    "        y_transforms = Compose(transforms)\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            transforms.insert(0, AddGaussianNoise(p=0.5))\n",
    "            transforms.insert(2, OneOf([T.TimeMasking(time_mask_param=100), T.FrequencyMasking(freq_mask_param=100)]))\n",
    "            \n",
    "        return Compose(transforms), y_transforms\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_path)\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # print(self.data_path[index])\n",
    "        with safe_open(self.data_path[index], framework=\"pt\", device=\"cpu\") as f:\n",
    "            sample_rate = f.get_tensor(\"sample_rate\")\n",
    "            audio = f.get_tensor(\"audio\")\n",
    "\n",
    "        x_transforms, y_transforms = self._get_transforms(sample_rate)\n",
    "        num_frames: int = audio.shape[1]\n",
    "        crop_frames: int = self.crop_size * sample_rate\n",
    "\n",
    "        frame_offset = -1\n",
    "        x_transformed: torch.Tensor\n",
    "        y_transformed: torch.Tensor\n",
    "        if num_frames > crop_frames:\n",
    "            while True:\n",
    "                frame_offset: int = random.randint(0, num_frames - crop_frames)\n",
    "                cropped_audio = audio[:, frame_offset : frame_offset + crop_frames]\n",
    "                x_transformed, y_transformed = x_transforms(cropped_audio), y_transforms(cropped_audio)\n",
    "                print(torch.isnan(x_transformed).sum(), torch.isnan(x_transformed).sum())\n",
    "                if not torch.isnan(x_transformed).sum() and not torch.isnan(x_transformed).sum():\n",
    "                    break\n",
    "            return x_transformed, y_transformed\n",
    "        else:\n",
    "            return x_transforms(audio), y_transforms(audio)\n",
    "        # return x_transforms(audio), y_transforms(audio), audio, frame_offset, crop_frames, num_frames"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T12:27:57.809835756Z",
     "start_time": "2023-12-31T12:27:57.804672017Z"
    }
   },
   "id": "3b8ec12c34c4cd57"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(\n",
    "#     dataset=AudioDataset(data_path=valid[178:179], image_size=256, mode=\"valid\"),\n",
    "#     batch_size=1,\n",
    "#     num_workers=0,\n",
    "#     shuffle=False,\n",
    "#     pin_memory=True,\n",
    "#     persistent_workers=False\n",
    "# )\n",
    "# for x, y in dataloader:\n",
    "#     print(x.min(), x.max())\n",
    "#     print()\n",
    "#     print(y.min(), y.max())\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-31T12:27:57.815061304Z",
     "start_time": "2023-12-31T12:27:57.812790836Z"
    }
   },
   "id": "7de1e1873773de00"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
