{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "SEED: int = 654\n",
    "\n",
    "def seed_everything(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    BATCH_SIZE: int = 8\n",
    "    IMAGE_SIZE: int = 256\n",
    "    EPOCHS: int = 100\n",
    "    LEARNING_RATE: float = 0.001\n",
    "    LEARNING_RATE_DECAY: int = 0\n",
    "    TRAIN_SIZE: float = 0.7\n",
    "    BASE_CHANNEL_SIZE: int = 16\n",
    "    LATENT_DIM: int = 128\n",
    "    NUM_INPUT_CHANNELS: int = 2\n",
    "\n",
    "\n",
    "cfg: Hyperparameters = Hyperparameters()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:44:31.305448763Z",
     "start_time": "2023-12-29T14:44:30.196487154Z"
    }
   },
   "id": "a846ed3e122d0282"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class StereoToMono(torch.nn.Module):\n",
    "    \"\"\"Convert audio from stereo to mono.\n",
    "    \"\"\"\n",
    "    def __init__(self, reduction: str = \"avg\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert isinstance(reduction, str)\n",
    "        assert reduction in [\"avg\", \"sum\"]\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, sample: torch.Tensor) -> torch.Tensor:\n",
    "        sample = sample.squeeze()\n",
    "        if sample.shape[0] == 1 or len(sample.shape) == 1:\n",
    "            return sample\n",
    "        return sample.mean(dim=0) if self.reduction == \"avg\" else sample.sum(dim=0)\n",
    "\n",
    "class AudioCrop(torch.nn.Module):\n",
    "    def __init__(self, sample_rate: int, crop_size: int = 60) -> None:\n",
    "        super().__init__()\n",
    "        self.crop_size: int = crop_size # in seconds\n",
    "        self.sample_rate: int = sample_rate\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if x.shape[1] <= self.crop_size * self.sample_rate:\n",
    "            return x\n",
    "        \n",
    "        start_frame: torch.Tensor = torch.randint(\n",
    "            low=0,\n",
    "            high=max(0, x.shape[1] - (self.crop_size*self.sample_rate)),\n",
    "            size=(1,)\n",
    "        ).detach()\n",
    "        return x[:, start_frame:start_frame + (self.crop_size * self.sample_rate)]\n",
    "\n",
    "class AddGaussianNoise(torch.nn.Module):\n",
    "    def __init__(self, mean: float = 0., std: float = 1., p: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        assert 0 <= p <= 1\n",
    "        self.std: float = std\n",
    "        self.mean: float = mean\n",
    "        self.p: float = p\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + torch.randn(x.size()) * self.std + self.mean if random.random() < self.p else x\n",
    "\n",
    "class Squeeze(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.squeeze(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:44:31.351490595Z",
     "start_time": "2023-12-29T14:44:31.350944624Z"
    }
   },
   "id": "64e8df48c50d79df"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paolo/git/spotify-playlist-generator/venv/lib/python3.11/site-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "from copy import deepcopy\n",
    "from torch_audiomentations import Compose, OneOf, SomeOf, Gain, HighPassFilter, LowPassFilter, PeakNormalization, PitchShift\n",
    "from torchvision.transforms import v2\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio\n",
    "import random\n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path: np.ndarray | list[str], image_size: int, sample_rate: int = 44100, crop_size: int = 60, mode: str = \"train\") -> None:\n",
    "        assert mode in [\"train\", \"valid\", \"test\"]\n",
    "        super().__init__()\n",
    "        self.data_path: np.ndarray | list[str] = data_path\n",
    "        self.image_size: int = image_size\n",
    "        self.sample_rate: int = sample_rate\n",
    "        self.crop_size: int = crop_size\n",
    "        self.mode: str = mode\n",
    "        self._init_transforms()\n",
    "        \n",
    "    def _init_transforms(self) -> None:\n",
    "        self.y_transforms = Compose([\n",
    "            T.MelSpectrogram(\n",
    "                sample_rate=self.sample_rate,\n",
    "                n_fft=512,\n",
    "                win_length=512,\n",
    "                hop_length=256,\n",
    "                n_mels=256\n",
    "            ),\n",
    "            v2.Resize(size=(self.image_size, self.image_size)),\n",
    "            v2.ToDtype(torch.float16, scale=True)\n",
    "        ])\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            self.x_transforms = Compose([\n",
    "                AddGaussianNoise(p=0.5),\n",
    "                T.MelSpectrogram(\n",
    "                    sample_rate=self.sample_rate,\n",
    "                    n_fft=512,\n",
    "                    win_length=512,\n",
    "                    hop_length=256,\n",
    "                    n_mels=256\n",
    "                ),\n",
    "                OneOf([\n",
    "                    T.TimeMasking(time_mask_param=100),\n",
    "                    T.FrequencyMasking(freq_mask_param=100)\n",
    "                ]),\n",
    "                v2.Resize(size=(self.image_size, self.image_size)),\n",
    "                v2.ToDtype(torch.float16, scale=True)\n",
    "            ])\n",
    "        else:\n",
    "            self.x_transforms = deepcopy(self.y_transforms)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data_path)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # print(self.data_path[index])\n",
    "        with safe_open(self.data_path[index], framework=\"pt\", device=\"cpu\") as f:\n",
    "            sample_rate = f.get_tensor(\"sample_rate\")\n",
    "            audio = f.get_tensor(\"audio\")\n",
    "\n",
    "        num_frames: int = audio.shape[1]\n",
    "        crop_frames: int = self.crop_size * sample_rate\n",
    "        # original = T.MelSpectrogram(\n",
    "        #     sample_rate=self.sample_rate,\n",
    "        #     n_fft=512,\n",
    "        #     win_length=512,\n",
    "        #     hop_length=256,\n",
    "        #     n_mels=256\n",
    "        # )(audio)\n",
    "        \n",
    "        frame_offset = -1\n",
    "        if num_frames > crop_frames:\n",
    "            frame_offset: int = random.randint(0, num_frames-crop_frames)\n",
    "            audio = audio[:, frame_offset:frame_offset+crop_frames]\n",
    "\n",
    "        # original_cropped = T.MelSpectrogram(\n",
    "        #     sample_rate=self.sample_rate,\n",
    "        #     n_fft=512,\n",
    "        #     win_length=512,\n",
    "        #     hop_length=256,\n",
    "        #     n_mels=256\n",
    "        # )(audio)\n",
    "\n",
    "        # print(f\"sample_rate: {sample_rate} - num_frames: {num_frames} - frame_offset: {frame_offset} - crop_frames: {crop_frames}\")\n",
    "        \n",
    "        return self.x_transforms(audio), self.y_transforms(audio)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:44:31.745688571Z",
     "start_time": "2023-12-29T14:44:31.351180669Z"
    }
   },
   "id": "6767515c2d59912c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train test split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f37d24479b3c4bc"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Any\n",
    "import pandas as pd\n",
    "\n",
    "def get_splits(data: pd.DataFrame | np.ndarray | list[...], train_size: float, valid_size: float, test_size: float, stratify_col: str | None = None) -> tuple[Any, Any, Any]:\n",
    "    assert train_size + valid_size + test_size <= 1.\n",
    "    \n",
    "    if stratify_col:\n",
    "        train_split, valid_test = train_test_split(data, train_size=train_size, stratify=data[stratify_col], random_state=SEED)\n",
    "        valid_split, test_split = train_test_split(valid_test, train_size=valid_size/(1-train_size), stratify=valid_test[stratify_col], random_state=SEED)\n",
    "    else:\n",
    "        train_split, valid_test = train_test_split(data, train_size=train_size, stratify=None, random_state=SEED)\n",
    "        valid_split, test_split = train_test_split(valid_test, train_size=valid_size/(1-train_size), stratify=None, random_state=SEED)\n",
    "        \n",
    "    return train_split, valid_split, test_split\n",
    "\n",
    "songs_path: list[pathlib.Path] = list(pathlib.Path(os.getcwd()).parent.rglob(\"*.safetensors\"))\n",
    "train, valid, test = get_splits(songs_path, train_size=0.7, valid_size=0.2, test_size=0.1, stratify_col=None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:44:32.464413156Z",
     "start_time": "2023-12-29T14:44:31.745333770Z"
    }
   },
   "id": "2ce83dd09ece6b4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62287949247dbba7"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: nn.Module = nn.Mish):\n",
    "        \"\"\"Encoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, base_channel_size, kernel_size=3, padding=1, stride=2),  # 256 => 128\n",
    "            act_fn(),\n",
    "            nn.Conv2d(base_channel_size, 2 * base_channel_size, kernel_size=3, padding=1, stride=2),  # 128 => 64\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * base_channel_size, 2 * base_channel_size, kernel_size=3, padding=1, stride=2), # 64 => 32,32,32\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * base_channel_size, 2 * base_channel_size, kernel_size=3, padding=1, stride=2),  # 32 => 32,16,16\n",
    "            act_fn(),\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(512 * base_channel_size, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"input shape {x.shape}\")\n",
    "        x = self.net(x)\n",
    "        # print(f\"encoder output shape {x.shape}\")\n",
    "        x = self.flatten(x)\n",
    "        # print(f\"flatten output shape {x.shape}\")\n",
    "        x = self.fc(x)\n",
    "        # print(f\"linear output shape {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: nn.Module = nn.Mish):\n",
    "        \"\"\"Decoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        # self.linear = nn.Sequential(nn.Linear(latent_dim, 2 * 16 * c_hid), act_fn())\n",
    "        self.linear = nn.Sequential(nn.Linear(latent_dim,512 * base_channel_size), act_fn())\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * base_channel_size, 2 * base_channel_size, kernel_size=3, padding=1, stride=2, output_padding=1),  # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2 * base_channel_size, 2 * base_channel_size, kernel_size=3, padding=1, stride=2, output_padding=1),  # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2 * base_channel_size, base_channel_size, kernel_size=3, padding=1, stride=2, output_padding=1), # 16x16 => 32x32\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(base_channel_size, num_input_channels, kernel_size=3, padding=1, stride=2, output_padding=1), # 16x16 => 32x32\n",
    "            act_fn(),\n",
    "            nn.Sigmoid(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f\"decoder input shape {x.shape}\")\n",
    "        x = self.linear(x)\n",
    "        # print(f\"decoder linear shape {x.shape}\")\n",
    "        x = x.reshape(x.shape[0], -1, 16, 16)\n",
    "        # print(f\"decoder reshape shape {x.shape}\")\n",
    "        x = self.net(x)\n",
    "        # print(f\"decoder output shape {x.shape}\")\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:44:32.475309419Z",
     "start_time": "2023-12-29T14:44:32.468508863Z"
    }
   },
   "id": "d02cb2987f5f991b"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ebd7c97c0a9e2b6c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import lightning as L\n",
    "\n",
    "class Autoencoder(L.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            base_channel_size: int,\n",
    "            latent_dim: int,\n",
    "            encoder_class: Encoder = Encoder,\n",
    "            decoder_class: Decoder = Decoder,\n",
    "            num_input_channels: int = 2,\n",
    "            width: int = cfg.IMAGE_SIZE,\n",
    "            height: int = cfg.IMAGE_SIZE,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.loss = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        # print(f\"input autoencoder shape {x.shape}\")\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case).\"\"\"\n",
    "        x, _ = batch  # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "        return self.loss(x_hat, x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=5, min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"valid_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        optimizer = self.optimizers()\n",
    "\n",
    "        # first forward-backward pass\n",
    "        loss_1 = self.compute_loss(batch)\n",
    "        self.manual_backward(loss_1, optimizer)\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "    \n",
    "        # second forward-backward pass\n",
    "        loss_2 = self.compute_loss(batch)\n",
    "        self.manual_backward(loss_2, optimizer)\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        \"\"\"\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"valid_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:44:33.039631724Z",
     "start_time": "2023-12-29T14:44:32.472452443Z"
    }
   },
   "id": "ba436d03de893976"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "import sys\n",
    "\n",
    "class MyProgressBar(TQDMProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = super().init_validation_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    def init_predict_tqdm(self):\n",
    "        bar = super().init_predict_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar\n",
    "\n",
    "    def init_test_tqdm(self):\n",
    "        bar = super().init_test_tqdm()\n",
    "        if not sys.stdout.isatty():\n",
    "            bar.disable = True\n",
    "        return bar"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:44:33.041417066Z",
     "start_time": "2023-12-29T14:44:33.040599642Z"
    }
   },
   "id": "fd53511fcb0b213e"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"valid_loss\", min_delta=0.00, patience=3, verbose=True, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='/home/paolo/git/spotify-playlist-generator/models', filename='{epoch}-{val_loss:.5f}', verbose=True, monitor=\"valid_loss\")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=AudioDataset(data_path=train, image_size=cfg.IMAGE_SIZE, mode=\"train\"),\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    num_workers=1,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    dataset=AudioDataset(data_path=valid, image_size=cfg.IMAGE_SIZE, mode=\"valid\"),\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    num_workers=1,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:44:33.092972874Z",
     "start_time": "2023-12-29T14:44:33.043516241Z"
    }
   },
   "id": "ad919695f12b874f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import librosa\n",
    "# \n",
    "# def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "#     if ax is None:\n",
    "#         _, ax = plt.subplots(1, 1)\n",
    "#     if title is not None:\n",
    "#         ax.set_title(title)\n",
    "#     ax.set_ylabel(ylabel)\n",
    "#     ax.imshow(librosa.power_to_db(specgram[0]), origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")\n",
    "# \n",
    "# idx = 0\n",
    "# for x, y, orig, orig_crop in valid_dataloader:\n",
    "#     x, y, orig, orig_crop = x[idx].numpy(), y[idx].numpy(), orig[idx].numpy(), orig_crop[idx].numpy()\n",
    "#     print(f\"x shape: {x.shape} - y shape: {y.shape} - orig shape: {orig.shape} - orig_crop shape: {orig_crop.shape}\")\n",
    "#     \n",
    "#     print(f\"x min: {x.min()} - x max: {x.max()} - x mean: {x.mean()}\")\n",
    "#     print(f\"y min: {y.min()} - y max: {y.max()} - y mean: {y.mean()}\")\n",
    "#     print(f\"orig min: {orig.min()} - orig max: {orig.max()} - orig mean: {orig.mean()}\")\n",
    "#     print(f\"orig_crop min: {orig_crop.min()} - orig_crop max: {orig_crop.max()} - orig_crop mean: {orig_crop.mean()}\")\n",
    "# \n",
    "#     plot_spectrogram(x, \"x\")\n",
    "#     plot_spectrogram(y, \"y\")\n",
    "#     plot_spectrogram(orig, \"orig\")\n",
    "#     plot_spectrogram(orig_crop, \"orig cropped\")\n",
    "#     break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T14:44:33.093389821Z",
     "start_time": "2023-12-29T14:44:33.090844502Z"
    }
   },
   "id": "afbdf514bcf255b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and is recommended only for model debugging.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": "┏━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓\n┃\u001B[1;35m \u001B[0m\u001B[1;35m \u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35mName   \u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35mType   \u001B[0m\u001B[1;35m \u001B[0m┃\u001B[1;35m \u001B[0m\u001B[1;35mParams\u001B[0m\u001B[1;35m \u001B[0m┃\n┡━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩\n│\u001B[2m \u001B[0m\u001B[2m0\u001B[0m\u001B[2m \u001B[0m│ encoder │ Encoder │  1.1 M │\n│\u001B[2m \u001B[0m\u001B[2m1\u001B[0m\u001B[2m \u001B[0m│ decoder │ Decoder │  1.1 M │\n│\u001B[2m \u001B[0m\u001B[2m2\u001B[0m\u001B[2m \u001B[0m│ loss    │ MSELoss │      0 │\n└───┴─────────┴─────────┴────────┘\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓\n┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n┡━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ encoder │ Encoder │  1.1 M │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ decoder │ Decoder │  1.1 M │\n│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ loss    │ MSELoss │      0 │\n└───┴─────────┴─────────┴────────┘\n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "\u001B[1mTrainable params\u001B[0m: 2.2 M                                                                                            \n\u001B[1mNon-trainable params\u001B[0m: 0                                                                                            \n\u001B[1mTotal params\u001B[0m: 2.2 M                                                                                                \n\u001B[1mTotal estimated model params size (MB)\u001B[0m: 8                                                                          \n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 2.2 M                                                                                            \n<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n<span style=\"font-weight: bold\">Total params</span>: 2.2 M                                                                                                \n<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 8                                                                          \n</pre>\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1b6d67b86af7443ca5b14d9f96c791e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import RichProgressBar\n",
    "\n",
    "model = Autoencoder(\n",
    "    base_channel_size=cfg.BASE_CHANNEL_SIZE,\n",
    "    latent_dim=cfg.LATENT_DIM,\n",
    "    encoder_class=Encoder,\n",
    "    decoder_class=Decoder,\n",
    ")\n",
    "\n",
    "\n",
    "trainer: L.Trainer = L.Trainer(\n",
    "    accelerator = \"gpu\",\n",
    "    num_nodes = 1,\n",
    "    precision = 16,\n",
    "    logger = None,\n",
    "    callbacks = [RichProgressBar()],\n",
    "    fast_dev_run = False,\n",
    "    max_epochs = cfg.EPOCHS,\n",
    "    min_epochs = 1,\n",
    "    overfit_batches = 1,\n",
    "    log_every_n_steps=100,\n",
    "    check_val_every_n_epoch = 1,\n",
    "    enable_checkpointing = False,\n",
    "    enable_progress_bar = True,\n",
    "    enable_model_summary = True,\n",
    "    deterministic = \"warn\",\n",
    "    benchmark = True,\n",
    "    inference_mode = True,\n",
    "    profiler = None,\n",
    "    detect_anomaly = True,\n",
    "    barebones = False,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=valid_dataloader,\n",
    "    ckpt_path=None\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-29T14:44:33.090939923Z"
    }
   },
   "id": "ae165a17496170ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import mlflow\n",
    "if False:\n",
    "    mlflow.pytorch.autolog(\n",
    "        log_every_n_epoch=1,\n",
    "        log_every_n_step=None,\n",
    "        log_models=True,\n",
    "        log_datasets=False,\n",
    "        disable=False,\n",
    "        exclusive=False,\n",
    "        disable_for_unsupported_versions=False,\n",
    "        silent=False,\n",
    "        registered_model_name=\"model\",\n",
    "        extra_tags=None\n",
    "    )\n",
    "    \n",
    "    model = Autoencoder(\n",
    "        base_channel_size=cfg.BASE_CHANNEL_SIZE,\n",
    "        latent_dim=cfg.LATENT_DIM,\n",
    "        encoder_class=Encoder,\n",
    "        decoder_class=Decoder,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    trainer: L.Trainer = L.Trainer(\n",
    "        accelerator = \"gpu\",\n",
    "        num_nodes = 1,\n",
    "        precision = 16,\n",
    "        logger = MLFlowLogger(experiment_name=\"lightning_experiment\"),\n",
    "        callbacks = [early_stop_callback, checkpoint_callback, MyProgressBar()],\n",
    "        fast_dev_run = False,\n",
    "        max_epochs = cfg.EPOCHS,\n",
    "        min_epochs = 1,\n",
    "        overfit_batches = 1,\n",
    "        log_every_n_steps=50,\n",
    "        check_val_every_n_epoch = 1,\n",
    "        enable_checkpointing = True,\n",
    "        enable_progress_bar = True,\n",
    "        enable_model_summary = True,\n",
    "        deterministic = \"warn\",\n",
    "        benchmark = True,\n",
    "        inference_mode = True,\n",
    "        profiler = None,\n",
    "        detect_anomaly = True,\n",
    "        barebones = False,\n",
    "    )\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        trainer.fit(\n",
    "            model=model,\n",
    "            train_dataloaders=train_dataloader,\n",
    "            val_dataloaders=valid_dataloader,\n",
    "            ckpt_path=None\n",
    "        )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fd2e5c63b048f533"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
